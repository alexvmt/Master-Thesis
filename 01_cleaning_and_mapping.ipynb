{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CLEANING AND MAPPING ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning and mapping...\n"
     ]
    }
   ],
   "source": [
    "print('Starting cleaning and mapping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2019-02-15 18:57:55.002326\n"
     ]
    }
   ],
   "source": [
    "### import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,date\n",
    "\n",
    "start_time = datetime.now()\n",
    "print('Start time: ', start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SELECT INPUT AND OUTPUT FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file selected:  3_day_sample_raw.tsv.gz\n",
      "Output file selected 3_day_sample_cleaned_and_mapped.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "input_file = '3_day_sample_raw.tsv.gz'\n",
    "output_file = '3_day_sample_cleaned_and_mapped.tsv.gz'\n",
    "#input_data = '6_week_sample_raw.tsv.gz'\n",
    "#output_data = '6_week_sample_cleaned_and_mapped.tsv.gz'\n",
    "#input_data = '12_week_sample_raw.tsv.gz'\n",
    "#output_data = '12_week_sample_cleaned_and_mapped.tsv.gz'\n",
    "#input_data = '25_week_sample_raw.tsv.gz'\n",
    "#output_data = '25_week_sample_cleaned_and_mapped.tsv.gz'\n",
    "\n",
    "print('Input file selected: ', input_file)\n",
    "print('Output file selected', output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "##### LOAD DATA\n",
    "print('Loading data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex.merdian-tarko\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:678: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data complete.\n"
     ]
    }
   ],
   "source": [
    "column_headers = pd.read_csv('../data/mapping_files/column_headers.tsv', sep='\\t')\n",
    "df = pd.read_csv('../data/raw_data/'+input_file, compression='gzip', sep='\\t', encoding='iso-8859-1', quoting=3, low_memory=False, names=column_headers)\n",
    "\n",
    "print('Loading data complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n"
     ]
    }
   ],
   "source": [
    "##### CLEAN DATA\n",
    "print('Cleaning data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows complete.\n"
     ]
    }
   ],
   "source": [
    "# reset index to make sure that index values are unique\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# drop rows where exclude_hit > 1\n",
    "df = df.drop(df[df.exclude_hit > 0].index)\n",
    "\n",
    "# drop rows where hit_source is 5, 7, 8 or 9\n",
    "df = df.drop(df[(df.hit_source == 5) | (df.hit_source == 7) | (df.hit_source == 8) | (df.hit_source == 9)].index)\n",
    "\n",
    "print('Dropping rows complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### browser mapping\n",
    "# load file for browser mapping\n",
    "browser_mapping = pd.read_csv('../data/mapping_files/browser_type.tsv', sep='\\t', header=None)\n",
    "browser_mapping.columns = ['browser_id', 'browser_name']\n",
    "\n",
    "# create dictionary for browser mapping\n",
    "browser_mapping_dict = dict(zip(browser_mapping.browser_id, browser_mapping.browser_name))\n",
    "\n",
    "# map browsers\n",
    "df['browser'] = df['browser'].map(browser_mapping_dict).fillna(df['browser'])\n",
    "df['browser'] = df['browser'].apply(lambda x: 'Not Specified' if x == 0 else x)\n",
    "\n",
    "print('Browser mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection type mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### connection type mapping\n",
    "# load file for connection type mapping\n",
    "connection_type_mapping = pd.read_csv('../data/mapping_files/connection_type.tsv', sep='\\t', header=None)\n",
    "connection_type_mapping.columns = ['connection_type_id', 'connection_type_name']\n",
    "\n",
    "# create dictionary for connection type mapping\n",
    "connection_type_mapping_dict = dict(zip(connection_type_mapping.connection_type_id, connection_type_mapping.connection_type_name))\n",
    "\n",
    "# map connection types\n",
    "df['connection_type'] = df['connection_type'].map(connection_type_mapping_dict).fillna(df['connection_type'])\n",
    "\n",
    "print('Connection type mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### country mapping\n",
    "# load file for country mapping\n",
    "country_mapping = pd.read_csv('../data/mapping_files/country.tsv', sep='\\t', header=None)\n",
    "country_mapping.columns = ['country_id', 'country_name']\n",
    "\n",
    "# drop dupliate countries\n",
    "country_mapping = country_mapping.drop_duplicates('country_name').reset_index(drop=True)\n",
    "\n",
    "# create dictionary for country mapping\n",
    "country_mapping_dict = dict(zip(country_mapping.country_id, country_mapping.country_name))\n",
    "\n",
    "# map countries\n",
    "df['country'] = df['country'].map(country_mapping_dict).fillna(df['country'])\n",
    "\n",
    "print('Country mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom evars mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### custom evars mapping\n",
    "# load file for custom evars mapping\n",
    "evars = pd.read_csv('../data/mapping_files/custom_evars.tsv', sep='\\t')\n",
    "evars_mapping = evars[['id', 'name']]\n",
    "\n",
    "# map custom evars\n",
    "evar_cols = [x for x in df.columns if x.lower()[:9] == 'post_evar']\n",
    "evar_cols = [x.replace('post_', '') for x in evar_cols]\n",
    "evars_mapped = evars[evars['id'].isin(evar_cols)][['id', 'name']]\n",
    "evars_mapped['id'] = evars_mapped['id'].apply(lambda x: 'post_' + x)\n",
    "evars_mapped = evars_mapped.reset_index(drop=True)\n",
    "\n",
    "# rename custom evars\n",
    "for i in range(evars_mapped.shape[0]):\n",
    "    df.rename(columns={evars_mapped.iloc[i,0] : str.lower(evars_mapped.iloc[i,1]).replace(' ','_')}, inplace=True)\n",
    "    \n",
    "print('Custom evars mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom marketing channel mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### custom marketing channel mapping\n",
    "# load file for marketing channel mapping\n",
    "marketing_channel_mapping = pd.read_csv('../data/mapping_files/custom_marketing_channels.tsv', sep='\\t')\n",
    "\n",
    "# create dictionary for marketing channel mapping\n",
    "marketing_channel_mapping_dict = dict(zip(marketing_channel_mapping.channel_id, marketing_channel_mapping.name))\n",
    "\n",
    "# map marketing channels\n",
    "df['marketing_channel'] = df['va_closer_id'].map(marketing_channel_mapping_dict).fillna(df['va_closer_id'])\n",
    "df['marketing_channel'] = df['marketing_channel'].apply(lambda x: 'Not Specified' if x == 0 else x)\n",
    "\n",
    "print('Custom marketing channel mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard and custom events mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### standard and custom events mapping\n",
    "# fill missing values in post_event_list\n",
    "df['post_event_list'] = df['post_event_list'].fillna('Not Specified')\n",
    "\n",
    "# load file for standard event mapping\n",
    "standard_events = pd.read_csv('../data/mapping_files/event.tsv', sep='\\t', header=None)\n",
    "standard_events.columns = ['event_id', 'event_name']\n",
    "\n",
    "# load file for custom event mapping\n",
    "custom_events = pd.read_csv('../data/mapping_files/custom_events.tsv', sep='\\t')\n",
    "custom_events['event_id'] = custom_events.index + 200\n",
    "\n",
    "# map standard and custom events\n",
    "events = pd.merge(standard_events, custom_events, how='inner', on='event_id')\n",
    "events_mapping = events[['event_id', 'name']]\n",
    "events_mapping = events_mapping.reset_index(drop=True)\n",
    "\n",
    "# create event dummies\n",
    "for id, event in zip(events_mapping.iloc[:,0], events_mapping.iloc[:,1]):\n",
    "        df[str.lower(event).replace(' ','_')] = df['post_event_list'].apply(lambda x: 1 if ','+str(id)+',' in x else 0)\n",
    "        \n",
    "# drop internal users\n",
    "df = df.drop(df[df['internal_user_(e30)'] == 1].index)\n",
    "\n",
    "print('Standard and custom events mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom props mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### custom props mapping\n",
    "# load file for custom props mapping\n",
    "props = pd.read_csv('../data/mapping_files/custom_props.tsv', sep='\\t')\n",
    "props_mapping = props[['id', 'name']]\n",
    "\n",
    "# map custom evars\n",
    "prop_cols = [x for x in df.columns if x.lower()[:9] == 'post_prop']\n",
    "prop_cols = [x.replace('post_', '') for x in prop_cols]\n",
    "props_mapped = props[props['id'].isin(prop_cols)][['id', 'name']]\n",
    "props_mapped['id'] = props_mapped['id'].apply(lambda x: 'post_' + x)\n",
    "props_mapped = props_mapped.reset_index(drop=True)\n",
    "\n",
    "# rename custom props\n",
    "for i in range(props_mapped.shape[0]):\n",
    "    df.rename(columns={props_mapped.iloc[i,0] : str.lower(props_mapped.iloc[i,1]).replace(' ','_')}, inplace=True)\n",
    "    \n",
    "print('Custom props mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating system mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### operating system mapping\n",
    "# load file for operating system mapping\n",
    "operating_system_mapping = pd.read_csv('../data/mapping_files/operating_systems.tsv', sep='\\t', header=None)\n",
    "operating_system_mapping.columns = ['operating_system_id', 'operating_system_name']\n",
    "\n",
    "# create dictionary for operating system mapping\n",
    "operating_system_mapping_dict = dict(zip(operating_system_mapping.operating_system_id, operating_system_mapping.operating_system_name))\n",
    "\n",
    "# map operating systems\n",
    "df['operating_system'] = df['os'].map(operating_system_mapping_dict).fillna(df['os'])\n",
    "df.drop('os', axis=1, inplace=True)\n",
    "\n",
    "# generalize operating system\n",
    "def generalize_operating_system(row):\n",
    "    if 'Windows' in row['operating_system']:\n",
    "        return 'Windows'\n",
    "    elif 'Linux' in row['operating_system']:\n",
    "        return 'Linux'\n",
    "    elif 'Android' in row['operating_system']:\n",
    "        return 'Android'\n",
    "    elif 'Mobile iOS' in row['operating_system']:\n",
    "        return 'Apple'\n",
    "    elif 'Macintosh' in row['operating_system']:\n",
    "        return 'Apple'\n",
    "    elif 'OS X' in row['operating_system']:\n",
    "        return 'Apple'\n",
    "    elif 'Not Specified' in row['operating_system']:\n",
    "        return 'Not Specified'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "df['operating_system_generalized'] = df.apply(generalize_operating_system, axis=1)\n",
    "\n",
    "print('Operating system mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Referrer type mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### referrer type mapping\n",
    "# load file for referrer type mapping\n",
    "referrer_type_mapping = pd.read_csv('../data/mapping_files/referrer_type.tsv', sep='\\t', header=None)\n",
    "referrer_type_mapping.columns = ['referrer_type_id', 'referrer_type_name', 'referrer_type']\n",
    "\n",
    "# create dictionary for referrer type mapping\n",
    "referrer_type_mapping_dict = dict(zip(referrer_type_mapping.referrer_type_id, referrer_type_mapping.referrer_type))\n",
    "\n",
    "# map referrer types\n",
    "df['referrer_type'] = df['ref_type'].map(referrer_type_mapping_dict).fillna(df['ref_type'])\n",
    "\n",
    "print('Referrer type mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search engine mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### search engine mapping\n",
    "# load file for search engine mapping\n",
    "search_engine_mapping = pd.read_csv('../data/mapping_files/search_engines.tsv', sep='\\t', header=None)\n",
    "search_engine_mapping.columns = ['search_engine_id', 'search_engine_name']\n",
    "\n",
    "# create dictionary for search engine mapping\n",
    "search_engine_mapping_dict = dict(zip(search_engine_mapping.search_engine_id, search_engine_mapping.search_engine_name))\n",
    "\n",
    "# map search engines\n",
    "df['search_engine'] = df['post_search_engine'].map(search_engine_mapping_dict).fillna(df['post_search_engine'])\n",
    "df.drop('post_search_engine', axis=1, inplace=True)\n",
    "\n",
    "# convert search engine column to string type\n",
    "df['search_engine'] = df['search_engine'].astype(str)\n",
    "\n",
    "# generalize search engine\n",
    "def generalize_search_engine(row):\n",
    "    if 'Google' in row['search_engine']:\n",
    "        return 'Google'\n",
    "    elif 'Yahoo' in row['search_engine']:\n",
    "        return 'Yahoo'\n",
    "    elif 'Bing' in row['search_engine']:\n",
    "        return 'Bing'\n",
    "    elif 'Baidu' in row['search_engine']:\n",
    "        return 'Baidu'\n",
    "    elif 'DuckDuckGo' in row['search_engine']:\n",
    "        return 'DuckDuckGo'\n",
    "    elif 'Yandex' in row['search_engine']:\n",
    "        return 'Yandex'\n",
    "    elif 'Search.ch' in row['search_engine']:\n",
    "        return 'Search.ch'\n",
    "    elif '0' in row['search_engine']:\n",
    "        return ' Not Specified'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "df['search_engine_generalized'] = df.apply(generalize_search_engine, axis=1)\n",
    "\n",
    "print('Search engine mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product item, prices and categories splitting complete.\n"
     ]
    }
   ],
   "source": [
    "### split and process product_items, product_item_price and product_categories columns\n",
    "df['num_product_items_seen'] = df['product_items'].apply(lambda x: 0 if pd.isnull(x) else len(x.split(';')))\n",
    "\n",
    "df['sum_price_product_items_seen'] = df['product_item_price'].apply(lambda x: 0 if pd.isnull(x)\n",
    "                                                              else sum([float(i) for i in x.split(';')]))\n",
    "\n",
    "df['product_categories_level_1'] = df['product_categories'].apply(lambda x: 'Not Specified' if pd.isnull(x)\n",
    "                                                                  else [i.split(' / ') for i in x.split(';')][0][0])\n",
    "\n",
    "df['product_categories_level_2'] = df['product_categories'].apply(lambda x: 'Not Specified' if pd.isnull(x) else \n",
    "                                                                  ([i.split(' / ') for i in x.split(';')][0][1] if len([i.split(' / ') for i in x.split(';')][0]) > 1 \n",
    "                                                                  else 'Not Specified'))\n",
    "\n",
    "df['product_categories_level_3'] = df['product_categories'].apply(lambda x: 'Not Specified' if pd.isnull(x) else \n",
    "                                                                  ([i.split(' / ') for i in x.split(';')][0][2] if len([i.split(' / ') for i in x.split(';')][0]) > 2 \n",
    "                                                                  else 'Not Specified'))\n",
    "\n",
    "print('Product item, prices and categories splitting complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing and faulty values complete.\n"
     ]
    }
   ],
   "source": [
    "### filling missing and faulty values\n",
    "df['cart_value_(v50)'].fillna(0, inplace=True)\n",
    "df['geo_city'] = df['geo_city'].apply(lambda x: 'Not Specified' if x == '?' else x)\n",
    "df['geo_region'] = df['geo_region'].apply(lambda x: 'Not Specified' if x == '?' else x)\n",
    "df['post_channel'] = df['post_channel'].fillna('Not Specified')\n",
    "df['post_cookies'] = df['post_cookies'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "df['post_persistent_cookie'] = df['post_persistent_cookie'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "df['registered_user'] = df['registered_user_(user)_(v34)'].apply(lambda x: 1 if x == 'y' else 0)\n",
    "df['login_status'] = df['login_status_(hit)_(v37)'].apply(lambda x: 1 if x == 'y' else 0)\n",
    "\n",
    "print('Filling missing and faulty values complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting data types complete.\n"
     ]
    }
   ],
   "source": [
    "### casting data types\n",
    "df['hit_time_gmt'] = pd.to_datetime(df['hit_time_gmt'], unit='s')\n",
    "df['visit_page_num'] = df['visit_page_num'].astype(np.int64)\n",
    "\n",
    "print('Casting data types complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns complete.\n"
     ]
    }
   ],
   "source": [
    "### drop some columns\n",
    "columns_to_keep = ['visitor_id', \n",
    "                   'hit_time_gmt',\n",
    "                   # numerical columns\n",
    "                   'visit_num', \n",
    "                   'visit_page_num', \n",
    "                   'purchase_boolean',\n",
    "                   'product_view_boolean', \n",
    "                   'checkout_boolean', \n",
    "                   'cart_addition_boolean', \n",
    "                   'cart_removal_boolean', \n",
    "                   'cart_view_boolean', \n",
    "                   'campaign_view_boolean', \n",
    "                   'cart_value_(v50)', \n",
    "                   'page_view_boolean', \n",
    "                   'last_purchase_num', \n",
    "                   'num_product_items_seen', \n",
    "                   'sum_price_product_items_seen', \n",
    "                   'server_call_counter_(e1)', \n",
    "                   'int._stand._search_result_clicked_(e16)', \n",
    "                   'active_stand._search_started_(e17)', \n",
    "                   'sugg._search_result_clicked_(e18)',\n",
    "                   # categorical columns\n",
    "                   'country', \n",
    "                   'geo_region', \n",
    "                   'geo_city', \n",
    "                   'geo_zip', \n",
    "                   'geo_dma',\n",
    "                   'post_channel', \n",
    "                   'post_cookies', \n",
    "                   'post_persistent_cookie', \n",
    "                   'search_page_num',\n",
    "                   'connection_type', \n",
    "                   'browser', \n",
    "                   'operating_system_generalized', \n",
    "                   'search_engine_generalized', \n",
    "                   'marketing_channel', \n",
    "                   'referrer_type', \n",
    "                   'repeat_orders_(e9)', \n",
    "                   'net_promoter_score_raw_(v10)_-_user', \n",
    "                   'registration_(any_form)_(e20)', \n",
    "                   'hit_of_logged_in_user_(e23)', \n",
    "                   'newsletter_signup_(any_form)_(e26)', \n",
    "                   'newsletter_subscriber_(e27)', \n",
    "                   'registered_user', \n",
    "                   'login_status', \n",
    "                   'user_gender_(v61)', \n",
    "                   'user_age_(v62)', \n",
    "                   'visit_during_tv_spot_(e71)', \n",
    "                   'login_success_(e72)', \n",
    "                   'logout_success_(e73)', \n",
    "                   'login_fail_(e74)', \n",
    "                   'registration_fail_(e75)',\n",
    "                   'new_visit', \n",
    "                   'hourly_visitor', \n",
    "                   'daily_visitor', \n",
    "                   'weekly_visitor', \n",
    "                   'monthly_visitor', \n",
    "                   'quarterly_visitor', \n",
    "                   'yearly_visitor', \n",
    "                   'product_categories_level_1', \n",
    "                   'product_categories_level_2', \n",
    "                   'product_categories_level_3']\n",
    "\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "### note on columns\n",
    "# open_cart is static/not filled\n",
    "# other static columns: registration_(any_form)_(e20), newsletter_signup_(any_form)_(e26), newsletter_subscriber_(e27), login_success_(e72), logout_success_(e73), login_fail_(e74), registration_fail_(e75)\n",
    "# columns with lots of missing values: net_promoter_score_raw_(v10)_-_user, user_gender_(v61), user_age_(v62)\n",
    "# unclear use: event level columns, post_channel\n",
    "\n",
    "print('Dropping columns complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataframe to file...\n"
     ]
    }
   ],
   "source": [
    "##### WRITE DATAFRAME TO FILE\n",
    "print('Writing dataframe to file...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed_data/'+output_file, compression='gzip', sep='\\t', encoding='iso-8859-1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and mapping complete.\n",
      "Run time:  0:15:07.549466\n"
     ]
    }
   ],
   "source": [
    "print('Cleaning and mapping complete.')\n",
    "print('Run time: ', datetime.now() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
