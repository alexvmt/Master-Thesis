{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## CLEANING AND MAPPING ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cleaning and mapping...\n"
     ]
    }
   ],
   "source": [
    "print('Starting cleaning and mapping...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2019-03-08 18:36:39.505798\n"
     ]
    }
   ],
   "source": [
    "### import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,date\n",
    "\n",
    "start_time = datetime.now()\n",
    "print('Start time: ', start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### SELECT INPUT AND OUTPUT FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input file selected:  3_day_sample_raw.tsv.gz\n",
      "Output file selected:  3_day_sample_cleaned_and_mapped.tsv.gz\n"
     ]
    }
   ],
   "source": [
    "input_file = '3_day_sample_raw.tsv.gz'\n",
    "output_file = '3_day_sample_cleaned_and_mapped.tsv.gz'\n",
    "#input_file = '6_week_sample_raw.tsv.gz'\n",
    "#output_file = '6_week_sample_cleaned_and_mapped.tsv.gz'\n",
    "#input_file = '12_week_sample_raw.tsv.gz'\n",
    "#output_file = '12_week_sample_cleaned_and_mapped.tsv.gz'\n",
    "#input_file = '25_week_sample_raw.tsv.gz'\n",
    "#output_file = '25_week_sample_cleaned_and_mapped.tsv.gz'\n",
    "\n",
    "print('Input file selected: ', input_file)\n",
    "print('Output file selected: ', output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "##### LOAD DATA\n",
    "print('Loading data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex.merdian-tarko\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:702: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data complete.\n"
     ]
    }
   ],
   "source": [
    "column_headers = pd.read_csv('../data/mapping_files/column_headers.tsv', sep='\\t')\n",
    "columns = ['exclude_hit',\n",
    "           'hit_source',\n",
    "           'browser',\n",
    "           'connection_type',\n",
    "           'country',\n",
    "           'va_closer_id',\n",
    "           'post_event_list',\n",
    "           'os',\n",
    "           'ref_type',\n",
    "           'post_search_engine',\n",
    "           'user_agent',\n",
    "           'product_items',\n",
    "           'product_item_price',\n",
    "           'product_categories',\n",
    "           'post_cookies',\n",
    "           'post_persistent_cookie',\n",
    "           'hit_time_gmt',\n",
    "           'date_time',\n",
    "           'visit_page_num',\n",
    "           'visitor_id',\n",
    "           'visit_num',\n",
    "           'search_page_num',\n",
    "           'new_visit', \n",
    "           'hourly_visitor', \n",
    "           'daily_visitor', \n",
    "           'weekly_visitor', \n",
    "           'monthly_visitor', \n",
    "           'quarterly_visitor', \n",
    "           'yearly_visitor',\n",
    "           'purchase_boolean',\n",
    "           'product_view_boolean', \n",
    "           'checkout_boolean', \n",
    "           'cart_addition_boolean', \n",
    "           'cart_removal_boolean', \n",
    "           'cart_view_boolean', \n",
    "           'campaign_view_boolean',\n",
    "           'page_view_boolean', \n",
    "           'last_purchase_num',\n",
    "           'post_evar10',\n",
    "           'post_evar34',\n",
    "           'post_evar50',\n",
    "           'post_evar61',\n",
    "           'post_evar62']\n",
    "df = pd.read_csv('../data/raw_data/'+input_file, compression='gzip', sep='\\t', encoding='iso-8859-1', quoting=3, low_memory=False, names=column_headers, usecols=columns)\n",
    "\n",
    "print('Loading data complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning data...\n"
     ]
    }
   ],
   "source": [
    "##### CLEAN DATA\n",
    "print('Cleaning data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping rows complete.\n"
     ]
    }
   ],
   "source": [
    "# reset index to make sure that index values are unique\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# drop rows where exclude_hit > 1\n",
    "df = df.drop(df[df.exclude_hit > 0].index)\n",
    "\n",
    "# drop rows where hit_source is 5, 7, 8 or 9\n",
    "df = df.drop(df[(df.hit_source == 5) | (df.hit_source == 7) | (df.hit_source == 8) | (df.hit_source == 9)].index)\n",
    "\n",
    "print('Dropping rows complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### browser mapping\n",
    "# load file for browser mapping\n",
    "browser_mapping = pd.read_csv('../data/mapping_files/browser_type.tsv', sep='\\t', header=None)\n",
    "browser_mapping.columns = ['browser_id', 'browser_name']\n",
    "\n",
    "# create dictionary for browser mapping\n",
    "browser_mapping_dict = dict(zip(browser_mapping.browser_id, browser_mapping.browser_name))\n",
    "\n",
    "# map browsers\n",
    "df['browser'] = df['browser'].map(browser_mapping_dict).fillna(df['browser'])\n",
    "df['browser'] = df['browser'].apply(lambda x: 'Unknown' if x == 0 else x)\n",
    "\n",
    "print('Browser mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection type mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### connection type mapping\n",
    "# load file for connection type mapping\n",
    "connection_type_mapping = pd.read_csv('../data/mapping_files/connection_type.tsv', sep='\\t', header=None)\n",
    "connection_type_mapping.columns = ['connection_type_id', 'connection_type_name']\n",
    "\n",
    "# create dictionary for connection type mapping\n",
    "connection_type_mapping_dict = dict(zip(connection_type_mapping.connection_type_id, connection_type_mapping.connection_type_name))\n",
    "\n",
    "# map connection types\n",
    "df['connection_type'] = df['connection_type'].map(connection_type_mapping_dict).fillna(df['connection_type'])\n",
    "\n",
    "print('Connection type mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Country mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### country mapping\n",
    "# load file for country mapping\n",
    "country_mapping = pd.read_csv('../data/mapping_files/country.tsv', sep='\\t', header=None)\n",
    "country_mapping.columns = ['country_id', 'country_name']\n",
    "\n",
    "# drop dupliate countries\n",
    "country_mapping = country_mapping.drop_duplicates('country_name').reset_index(drop=True)\n",
    "\n",
    "# create dictionary for country mapping\n",
    "country_mapping_dict = dict(zip(country_mapping.country_id, country_mapping.country_name))\n",
    "\n",
    "# map countries\n",
    "df['country'] = df['country'].map(country_mapping_dict).fillna(df['country'])\n",
    "\n",
    "print('Country mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom evars mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### custom evars mapping\n",
    "# load file for custom evars mapping\n",
    "evars = pd.read_csv('../data/mapping_files/custom_evars.tsv', sep='\\t')\n",
    "evars_mapping = evars[['id', 'name']]\n",
    "\n",
    "# map custom evars\n",
    "evar_cols = [x for x in df.columns if x.lower()[:9] == 'post_evar']\n",
    "evar_cols = [x.replace('post_', '') for x in evar_cols]\n",
    "evars_mapped = evars[evars['id'].isin(evar_cols)][['id', 'name']]\n",
    "evars_mapped['id'] = evars_mapped['id'].apply(lambda x: 'post_' + x)\n",
    "evars_mapped = evars_mapped.reset_index(drop=True)\n",
    "\n",
    "# rename custom evars\n",
    "for i in range(evars_mapped.shape[0]):\n",
    "    df.rename(columns={evars_mapped.iloc[i,0] : str.lower(evars_mapped.iloc[i,1]).replace(' ','_')}, inplace=True)\n",
    "    \n",
    "print('Custom evars mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom marketing channel mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### custom marketing channel mapping\n",
    "# load file for marketing channel mapping\n",
    "marketing_channel_mapping = pd.read_csv('../data/mapping_files/custom_marketing_channels.tsv', sep='\\t')\n",
    "\n",
    "# create dictionary for marketing channel mapping\n",
    "marketing_channel_mapping_dict = dict(zip(marketing_channel_mapping.channel_id, marketing_channel_mapping.name))\n",
    "\n",
    "# map marketing channels\n",
    "df['va_closer_id'] = df['va_closer_id'].map(marketing_channel_mapping_dict).fillna(df['va_closer_id'])\n",
    "df['va_closer_id'] = df['va_closer_id'].apply(lambda x: 'Unknown' if x == 0 else x)\n",
    "\n",
    "print('Custom marketing channel mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard and custom events mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### standard and custom events mapping\n",
    "# fill missing values in post_event_list\n",
    "df['post_event_list'] = df['post_event_list'].fillna('Unknown')\n",
    "\n",
    "# load file for standard event mapping\n",
    "standard_events = pd.read_csv('../data/mapping_files/event.tsv', sep='\\t', header=None)\n",
    "standard_events.columns = ['event_id', 'event_name']\n",
    "\n",
    "# load file for custom event mapping\n",
    "custom_events = pd.read_csv('../data/mapping_files/custom_events.tsv', sep='\\t')\n",
    "custom_events['event_id'] = custom_events.index + 200\n",
    "\n",
    "# map standard and custom events\n",
    "events = pd.merge(standard_events, custom_events, how='inner', on='event_id')\n",
    "events_mapping = events[['event_id', 'name']]\n",
    "events_mapping = events_mapping.reset_index(drop=True)\n",
    "\n",
    "# create event dummies\n",
    "for id, event in zip(events_mapping.iloc[:,0], events_mapping.iloc[:,1]):\n",
    "        df[str.lower(event).replace(' ','_')] = df['post_event_list'].apply(lambda x: 1 if ','+str(id)+',' in x else 0)\n",
    "        \n",
    "# drop internal users\n",
    "df = df.drop(df[df['internal_user_(e30)'] == 1].index)\n",
    "\n",
    "print('Standard and custom events mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating system mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### operating system mapping\n",
    "# load file for operating system mapping\n",
    "operating_system_mapping = pd.read_csv('../data/mapping_files/operating_systems.tsv', sep='\\t', header=None)\n",
    "operating_system_mapping.columns = ['operating_system_id', 'operating_system_name']\n",
    "\n",
    "# create dictionary for operating system mapping\n",
    "operating_system_mapping_dict = dict(zip(operating_system_mapping.operating_system_id, operating_system_mapping.operating_system_name))\n",
    "\n",
    "# map operating systems\n",
    "df['os'] = df['os'].map(operating_system_mapping_dict).fillna(df['os'])\n",
    "\n",
    "# generalize operating system\n",
    "def generalize_operating_system(row):\n",
    "    if 'Windows' in row['os']:\n",
    "        return 'Windows'\n",
    "    elif 'Linux' in row['os']:\n",
    "        return 'Linux'\n",
    "    elif 'Android' in row['os']:\n",
    "        return 'Android'\n",
    "    elif 'Mobile iOS' in row['os']:\n",
    "        return 'Apple'\n",
    "    elif 'Macintosh' in row['os']:\n",
    "        return 'Apple'\n",
    "    elif 'OS X' in row['os']:\n",
    "        return 'Apple'\n",
    "    elif 'Not Specified' in row['os']:\n",
    "        return 'Unknown'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "df['operating_system_generalized'] = df.apply(generalize_operating_system, axis=1)\n",
    "\n",
    "print('Operating system mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Referrer type mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### referrer type mapping\n",
    "# load file for referrer type mapping\n",
    "referrer_type_mapping = pd.read_csv('../data/mapping_files/referrer_type.tsv', sep='\\t', header=None)\n",
    "referrer_type_mapping.columns = ['referrer_type_id', 'referrer_type_name', 'referrer_type']\n",
    "\n",
    "# create dictionary for referrer type mapping\n",
    "referrer_type_mapping_dict = dict(zip(referrer_type_mapping.referrer_type_id, referrer_type_mapping.referrer_type))\n",
    "\n",
    "# map referrer types\n",
    "df['ref_type'] = df['ref_type'].map(referrer_type_mapping_dict).fillna(df['ref_type'])\n",
    "\n",
    "print('Referrer type mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search engine mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### search engine mapping\n",
    "# load file for search engine mapping\n",
    "search_engine_mapping = pd.read_csv('../data/mapping_files/search_engines.tsv', sep='\\t', header=None)\n",
    "search_engine_mapping.columns = ['search_engine_id', 'search_engine_name']\n",
    "\n",
    "# create dictionary for search engine mapping\n",
    "search_engine_mapping_dict = dict(zip(search_engine_mapping.search_engine_id, search_engine_mapping.search_engine_name))\n",
    "\n",
    "# map search engines\n",
    "df['post_search_engine'] = df['post_search_engine'].map(search_engine_mapping_dict).fillna(df['post_search_engine'])\n",
    "\n",
    "# convert search engine column to string type\n",
    "df['post_search_engine'] = df['post_search_engine'].astype(str)\n",
    "\n",
    "# generalize search engine\n",
    "def generalize_search_engine(row):\n",
    "    if 'Google' in row['post_search_engine']:\n",
    "        return 'Google'\n",
    "    elif 'googleadservices.com' in row['post_search_engine']:\n",
    "        return 'Google'\n",
    "    elif 'Yahoo' in row['post_search_engine']:\n",
    "        return 'Yahoo'\n",
    "    elif 'Bing' in row['post_search_engine']:\n",
    "        return 'Bing'\n",
    "    elif 'Baidu' in row['post_search_engine']:\n",
    "        return 'Baidu'\n",
    "    elif 'DuckDuckGo' in row['post_search_engine']:\n",
    "        return 'DuckDuckGo'\n",
    "    elif 'Yandex' in row['post_search_engine']:\n",
    "        return 'Yandex'\n",
    "    elif 'Search.ch' in row['post_search_engine']:\n",
    "        return 'Search.ch'\n",
    "    elif '0' in row['post_search_engine']:\n",
    "        return 'Unknown'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "df['search_engine_generalized'] = df.apply(generalize_search_engine, axis=1)\n",
    "\n",
    "print('Search engine mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User agent mapping complete.\n"
     ]
    }
   ],
   "source": [
    "### user_agent mapping\n",
    "# fill missing values\n",
    "df['user_agent'] = df['user_agent'].fillna('Unknown')\n",
    "\n",
    "user_agent_mapping_file = 'user_agent_mapping.tsv.gz'\n",
    "\n",
    "# load file for user agent mapping\n",
    "user_agent_mapping = pd.read_csv('../data/mapping_files/'+user_agent_mapping_file, compression='gzip', sep='\\t', encoding='iso-8859-1', quoting=3, low_memory=False)\n",
    "\n",
    "# merge user agent mapping and df\n",
    "df = pd.merge(df, user_agent_mapping, how='left', on='user_agent')\n",
    "\n",
    "# drop rows where device_is_bot_user_agent == 1\n",
    "df = df.drop(df[df.device_is_bot_user_agent == 1].index)\n",
    "\n",
    "print('User agent mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product item, prices and categories splitting complete.\n"
     ]
    }
   ],
   "source": [
    "### split and process product_items, product_item_price and product_categories columns\n",
    "df['num_product_items_seen'] = df['product_items'].apply(lambda x: 0 if pd.isnull(x) else len(x.split(';')))\n",
    "\n",
    "df['sum_price_product_items_seen'] = df['product_item_price'].apply(lambda x: 0 if pd.isnull(x)\n",
    "                                                              else sum([float(i) for i in x.split(';')]))\n",
    "\n",
    "df['product_categories_level_1'] = df['product_categories'].apply(lambda x: 'Unknown' if pd.isnull(x)\n",
    "                                                                  else [i.split(' / ') for i in x.split(';')][0][0])\n",
    "\n",
    "df['product_categories_level_2'] = df['product_categories'].apply(lambda x: 'Unknown' if pd.isnull(x) else \n",
    "                                                                  ([i.split(' / ') for i in x.split(';')][0][1] if len([i.split(' / ') for i in x.split(';')][0]) > 1 \n",
    "                                                                  else 'Unknown'))\n",
    "\n",
    "df['product_categories_level_3'] = df['product_categories'].apply(lambda x: 'Unknown' if pd.isnull(x) else \n",
    "                                                                  ([i.split(' / ') for i in x.split(';')][0][2] if len([i.split(' / ') for i in x.split(';')][0]) > 2 \n",
    "                                                                  else 'Unknown'))\n",
    "\n",
    "print('Product item, prices and categories splitting complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling missing and faulty values complete.\n"
     ]
    }
   ],
   "source": [
    "### filling missing and faulty values\n",
    "df['cart_value_(v50)'].fillna(0, inplace=True)\n",
    "df['post_cookies'] = df['post_cookies'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "df['post_persistent_cookie'] = df['post_persistent_cookie'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "df['registered_user_(user)_(v34)'] = df['registered_user_(user)_(v34)'].apply(lambda x: 1 if x == 'y' else 0)\n",
    "\n",
    "print('Filling missing and faulty values complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Casting data types complete.\n"
     ]
    }
   ],
   "source": [
    "### casting data types\n",
    "df['hit_time_gmt'] = pd.to_datetime(df['hit_time_gmt'], unit='s')\n",
    "df['date_time'] = df['date_time'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "df['visit_page_num'] = df['visit_page_num'].astype(np.int64)\n",
    "\n",
    "print('Casting data types complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renaming and dropping columns complete.\n"
     ]
    }
   ],
   "source": [
    "### rename some columns\n",
    "df.rename(columns={'va_closer_id' : 'marketing_channel'}, inplace=True)\n",
    "df.rename(columns={'os' : 'operating_system'}, inplace=True)\n",
    "df.rename(columns={'ref_type' : 'referrer_type'}, inplace=True)\n",
    "df.rename(columns={'post_search_engine' : 'search_engine'}, inplace=True)\n",
    "df.rename(columns={'cart_value_(v50)' : 'cart_value'}, inplace=True)\n",
    "df.rename(columns={'server_call_counter_(e1)' : 'hit_counter'}, inplace=True)\n",
    "df.rename(columns={'int._stand._search_result_clicked_(e16)' : 'standard_search_results_clicked'}, inplace=True)\n",
    "df.rename(columns={'active_stand._search_started_(e17)' : 'standard_search_started'}, inplace=True)\n",
    "df.rename(columns={'sugg._search_result_clicked_(e18)' : 'suggested_search_results_clicked'}, inplace=True)\n",
    "df.rename(columns={'post_cookies' : 'cookies'}, inplace=True)\n",
    "df.rename(columns={'post_persistent_cookie' : 'persistent_cookie'}, inplace=True)\n",
    "df.rename(columns={'repeat_orders_(e9)' : 'repeat_orders'}, inplace=True)\n",
    "df.rename(columns={'net_promoter_score_raw_(v10)_-_user' : 'net_promoter_score'}, inplace=True)\n",
    "df.rename(columns={'hit_of_logged_in_user_(e23)' : 'hit_of_logged_in_user'}, inplace=True)\n",
    "df.rename(columns={'registered_user_(user)_(v34)' : 'registered_user'}, inplace=True)\n",
    "df.rename(columns={'user_gender_(v61)' : 'user_gender'}, inplace=True)\n",
    "df.rename(columns={'user_age_(v62)' : 'user_age'}, inplace=True)\n",
    "df.rename(columns={'visit_during_tv_spot_(e71)' : 'visit_during_tv_spot'}, inplace=True)\n",
    "\n",
    "columns_to_keep = ['visitor_id', \n",
    "                   'hit_time_gmt',\n",
    "                   'date_time',\n",
    "                   # numerical columns\n",
    "                   'visit_num', \n",
    "                   'visit_page_num', \n",
    "                   'purchase_boolean',\n",
    "                   'product_view_boolean', \n",
    "                   'checkout_boolean', \n",
    "                   'cart_addition_boolean', \n",
    "                   'cart_removal_boolean', \n",
    "                   'cart_view_boolean', \n",
    "                   'campaign_view_boolean', \n",
    "                   'cart_value', \n",
    "                   'page_view_boolean', \n",
    "                   'last_purchase_num', \n",
    "                   'num_product_items_seen', \n",
    "                   'sum_price_product_items_seen', \n",
    "                   'hit_counter', \n",
    "                   'standard_search_results_clicked', \n",
    "                   'standard_search_started', \n",
    "                   'suggested_search_results_clicked',\n",
    "                   # categorical columns\n",
    "                   'country', \n",
    "                   'cookies', \n",
    "                   'persistent_cookie', \n",
    "                   'search_page_num',\n",
    "                   'connection_type', \n",
    "                   'browser', \n",
    "                   'operating_system', \n",
    "                   'search_engine', \n",
    "                   'search_engine_generalized',\n",
    "                   'marketing_channel', \n",
    "                   'referrer_type', \n",
    "                   'new_visit', \n",
    "                   'hourly_visitor', \n",
    "                   'daily_visitor', \n",
    "                   'weekly_visitor', \n",
    "                   'monthly_visitor', \n",
    "                   'quarterly_visitor', \n",
    "                   'yearly_visitor', \n",
    "                   'product_categories_level_1', \n",
    "                   'product_categories_level_2', \n",
    "                   'product_categories_level_3', \n",
    "                   'device_type_user_agent', \n",
    "                   'device_brand_name_user_agent', \n",
    "                   'device_operating_system_user_agent', \n",
    "                   'device_browser_user_agent',\n",
    "                   'repeat_orders', \n",
    "                   'net_promoter_score', \n",
    "                   'hit_of_logged_in_user',\n",
    "                   'registered_user',\n",
    "                   'user_gender', \n",
    "                   'user_age', \n",
    "                   'visit_during_tv_spot']\n",
    "\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "print('Renaming and dropping columns complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing dataframe to file...\n"
     ]
    }
   ],
   "source": [
    "##### WRITE DATAFRAME TO FILE\n",
    "print('Writing dataframe to file...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed_data/'+output_file, compression='gzip', sep='\\t', encoding='iso-8859-1', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning and mapping complete.\n",
      "Run time:  0:13:18.560186\n"
     ]
    }
   ],
   "source": [
    "print('Cleaning and mapping complete.')\n",
    "run_time = datetime.now() - start_time\n",
    "print('Run time: ', run_time)\n",
    "\n",
    "run_time_dict_file = '3_day_sample_cleaning_and_mapping_run_time.txt'\n",
    "#run_time_dict_file = '6_week_sample_cleaning_and_mapping_run_time.txt'\n",
    "#run_time_dict_file = '12_week_sample_cleaning_and_mapping_run_time.txt'\n",
    "#run_time_dict_file = '25_week_sample_cleaning_and_mapping_run_time.txt'\n",
    "\n",
    "run_time_dict = {'cleaning and mapping run time' : run_time}\n",
    "\n",
    "f = open('../results/descriptives/'+run_time_dict_file, 'w')\n",
    "f.write(str(run_time_dict))\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
