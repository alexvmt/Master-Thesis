{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### HELPER FUNCTIONS #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime,date\n",
    "from device_detector import DeviceDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOADING AND WRITING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_file):\n",
    "    \n",
    "    print('Starting loading data...')\n",
    "    \n",
    "    # load column headers\n",
    "    column_headers = pd.read_csv('../data/mapping_files/column_headers.tsv', sep='\\t')\n",
    "    \n",
    "    # select columns\n",
    "    columns = ['exclude_hit',\n",
    "               'hit_source',\n",
    "               'browser',\n",
    "               'connection_type',\n",
    "               'country',\n",
    "               'va_closer_id',\n",
    "               'post_event_list',\n",
    "               'os',\n",
    "               'ref_type',\n",
    "               'post_search_engine',\n",
    "               'user_agent',\n",
    "               'product_items',\n",
    "               'product_item_price',\n",
    "               'product_categories',\n",
    "               'post_cookies',\n",
    "               'post_persistent_cookie',\n",
    "               'hit_time_gmt',\n",
    "               'date_time',\n",
    "               'visit_page_num',\n",
    "               'visitor_id',\n",
    "               'visit_num',\n",
    "               'search_page_num',\n",
    "               'new_visit', \n",
    "               'hourly_visitor', \n",
    "               'daily_visitor', \n",
    "               'weekly_visitor', \n",
    "               'monthly_visitor', \n",
    "               'quarterly_visitor', \n",
    "               'yearly_visitor',\n",
    "               'purchase_boolean',\n",
    "               'product_view_boolean', \n",
    "               'checkout_boolean', \n",
    "               'cart_addition_boolean', \n",
    "               'cart_removal_boolean', \n",
    "               'cart_view_boolean', \n",
    "               'campaign_view_boolean',\n",
    "               'page_view_boolean', \n",
    "               'last_purchase_num',\n",
    "               'post_evar10',\n",
    "               'post_evar34',\n",
    "               'post_evar50',\n",
    "               'post_evar61',\n",
    "               'post_evar62']\n",
    "    \n",
    "    # load data\n",
    "    df = pd.read_csv('../data/raw_data/'+input_file, compression='gzip', sep='\\t', encoding='iso-8859-1', quoting=3, low_memory=False, names=column_headers, usecols=columns)\n",
    "\n",
    "    print('Loading data complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data(df, output_file):\n",
    "    \n",
    "    print('Starting writing data...')\n",
    "    \n",
    "    # append chunks to final file   \n",
    "    df.to_csv('../data/processed_data/'+output_file, compression='gzip', sep='\\t', encoding='iso-8859-1', index=False, mode='a')\n",
    "    \n",
    "    print('Writing data complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CLEANING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_rows(df):\n",
    "    \n",
    "    print('Starting dropping rows...')\n",
    "    \n",
    "    # reset index to make sure that index values are unique\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # drop rows where exclude_hit > 1\n",
    "    df = df.drop(df[df.exclude_hit > 0].index)\n",
    "\n",
    "    # drop rows where hit_source is 5, 7, 8 or 9\n",
    "    df = df.drop(df[(df.hit_source == 5) | (df.hit_source == 7) | (df.hit_source == 8) | (df.hit_source == 9)].index)\n",
    "    \n",
    "    print('Dropping rows complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns(df):\n",
    "    \n",
    "    print('Starting dropping columns...')\n",
    "\n",
    "    # select columns to keep\n",
    "    columns_to_keep = ['visitor_id', \n",
    "                       'hit_time_gmt',\n",
    "                       'date_time',\n",
    "                       # numerical columns\n",
    "                       'visit_num', \n",
    "                       'visit_page_num', \n",
    "                       'purchase_boolean',\n",
    "                       'product_view_boolean', \n",
    "                       'checkout_boolean', \n",
    "                       'cart_addition_boolean', \n",
    "                       'cart_removal_boolean', \n",
    "                       'cart_view_boolean', \n",
    "                       'campaign_view_boolean', \n",
    "                       'cart_value', \n",
    "                       'page_view_boolean', \n",
    "                       'last_purchase_num', \n",
    "                       'num_product_items_seen', \n",
    "                       'sum_price_product_items_seen', \n",
    "                       'hit_counter', \n",
    "                       'standard_search_results_clicked', \n",
    "                       'standard_search_started', \n",
    "                       'suggested_search_results_clicked',\n",
    "                       # categorical columns\n",
    "                       'country', \n",
    "                       'cookies', \n",
    "                       'persistent_cookie', \n",
    "                       'search_page_num',\n",
    "                       'connection_type', \n",
    "                       'browser', \n",
    "                       'operating_system', \n",
    "                       'search_engine', \n",
    "                       'search_engine_generalized',\n",
    "                       'marketing_channel', \n",
    "                       'referrer_type', \n",
    "                       'new_visit', \n",
    "                       'hourly_visitor', \n",
    "                       'daily_visitor', \n",
    "                       'weekly_visitor', \n",
    "                       'monthly_visitor', \n",
    "                       'quarterly_visitor', \n",
    "                       'yearly_visitor', \n",
    "                       'product_categories_level_1', \n",
    "                       'product_categories_level_2', \n",
    "                       'product_categories_level_3', \n",
    "                       'device_type_user_agent', \n",
    "                       'device_brand_name_user_agent', \n",
    "                       'device_operating_system_user_agent', \n",
    "                       'device_browser_user_agent',\n",
    "                       'repeat_orders', \n",
    "                       'net_promoter_score', \n",
    "                       'hit_of_logged_in_user',\n",
    "                       'registered_user',\n",
    "                       'user_gender', \n",
    "                       'user_age', \n",
    "                       'visit_during_tv_spot']\n",
    "\n",
    "    # subset dataframe to select only columns to keep\n",
    "    df = df[columns_to_keep]\n",
    "\n",
    "    print('Dropping columns complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_missing_and_faulty_values(df):\n",
    "    \n",
    "    print('Starting filling missing and faulty values...')\n",
    "\n",
    "    # fill missing values\n",
    "    df['cart_value_(v50)'].fillna(0, inplace=True)\n",
    "    df['post_cookies'] = df['post_cookies'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "    df['post_persistent_cookie'] = df['post_persistent_cookie'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "    df['registered_user_(user)_(v34)'] = df['registered_user_(user)_(v34)'].apply(lambda x: 1 if x == 'y' else 0)\n",
    "\n",
    "    print('Filling missing and faulty values complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_data_types(df):\n",
    "    \n",
    "    print('Starting casting data types...')\n",
    "    \n",
    "    # cast data types\n",
    "    df['hit_time_gmt'] = pd.to_datetime(df['hit_time_gmt'], unit='s')\n",
    "    df['date_time'] = df['date_time'].apply(lambda x: datetime.strptime(x, '%Y-%m-%d %H:%M:%S'))\n",
    "    df['visit_page_num'] = df['visit_page_num'].apply(lambda x: np.nan if pd.isnull(x) else x.astype(np.int64))\n",
    "    df['post_search_engine'] = df['post_search_engine'].astype(str)\n",
    "\n",
    "    print('Casting data types complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    \n",
    "    print('Starting renaming columns...')\n",
    "    \n",
    "    # rename columns\n",
    "    df.rename(columns={'va_closer_id' : 'marketing_channel'}, inplace=True)\n",
    "    df.rename(columns={'os' : 'operating_system'}, inplace=True)\n",
    "    df.rename(columns={'ref_type' : 'referrer_type'}, inplace=True)\n",
    "    df.rename(columns={'post_search_engine' : 'search_engine'}, inplace=True)\n",
    "    df.rename(columns={'cart_value_(v50)' : 'cart_value'}, inplace=True)\n",
    "    df.rename(columns={'server_call_counter_(e1)' : 'hit_counter'}, inplace=True)\n",
    "    df.rename(columns={'int._stand._search_result_clicked_(e16)' : 'standard_search_results_clicked'}, inplace=True)\n",
    "    df.rename(columns={'active_stand._search_started_(e17)' : 'standard_search_started'}, inplace=True)\n",
    "    df.rename(columns={'sugg._search_result_clicked_(e18)' : 'suggested_search_results_clicked'}, inplace=True)\n",
    "    df.rename(columns={'post_cookies' : 'cookies'}, inplace=True)\n",
    "    df.rename(columns={'post_persistent_cookie' : 'persistent_cookie'}, inplace=True)\n",
    "    df.rename(columns={'repeat_orders_(e9)' : 'repeat_orders'}, inplace=True)\n",
    "    df.rename(columns={'net_promoter_score_raw_(v10)_-_user' : 'net_promoter_score'}, inplace=True)\n",
    "    df.rename(columns={'hit_of_logged_in_user_(e23)' : 'hit_of_logged_in_user'}, inplace=True)\n",
    "    df.rename(columns={'registered_user_(user)_(v34)' : 'registered_user'}, inplace=True)\n",
    "    df.rename(columns={'user_gender_(v61)' : 'user_gender'}, inplace=True)\n",
    "    df.rename(columns={'user_age_(v62)' : 'user_age'}, inplace=True)\n",
    "    df.rename(columns={'visit_during_tv_spot_(e71)' : 'visit_during_tv_spot'}, inplace=True)\n",
    "\n",
    "    print('Renaming columns complete')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAPPING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def browser_mapping(df):\n",
    "\n",
    "    print('Starting browser mapping...')\n",
    "    \n",
    "    # load file for browser mapping and select columns\n",
    "    browser_mapping = pd.read_csv('../data/mapping_files/browser_type.tsv', sep='\\t', header=None)\n",
    "    browser_mapping.columns = ['browser_id', 'browser_name']\n",
    "\n",
    "    # create dictionary for browser mapping\n",
    "    browser_mapping_dict = dict(zip(browser_mapping.browser_id, browser_mapping.browser_name))\n",
    "\n",
    "    # map browsers\n",
    "    df['browser'] = df['browser'].map(browser_mapping_dict).fillna(df['browser'])\n",
    "    df['browser'] = df['browser'].apply(lambda x: 'Unknown' if x == 0 else x)\n",
    "\n",
    "    print('Browser mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connection_type_mapping(df):\n",
    "    \n",
    "    print('Starting connection type mapping...')\n",
    "\n",
    "    # load file for connection type mapping and select columns\n",
    "    connection_type_mapping = pd.read_csv('../data/mapping_files/connection_type.tsv', sep='\\t', header=None)\n",
    "    connection_type_mapping.columns = ['connection_type_id', 'connection_type_name']\n",
    "\n",
    "    # create dictionary for connection type mapping\n",
    "    connection_type_mapping_dict = dict(zip(connection_type_mapping.connection_type_id, connection_type_mapping.connection_type_name))\n",
    "\n",
    "    # map connection types\n",
    "    df['connection_type'] = df['connection_type'].map(connection_type_mapping_dict).fillna(df['connection_type'])\n",
    "\n",
    "    print('Connection type mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def country_mapping(df):\n",
    "    \n",
    "    print('Starting country mapping...')\n",
    "    \n",
    "    # load file for country mapping and select columns\n",
    "    country_mapping = pd.read_csv('../data/mapping_files/country.tsv', sep='\\t', header=None)\n",
    "    country_mapping.columns = ['country_id', 'country_name']\n",
    "\n",
    "    # drop dupliate countries\n",
    "    country_mapping = country_mapping.drop_duplicates('country_name').reset_index(drop=True)\n",
    "\n",
    "    # create dictionary for country mapping\n",
    "    country_mapping_dict = dict(zip(country_mapping.country_id, country_mapping.country_name))\n",
    "\n",
    "    # map countries\n",
    "    df['country'] = df['country'].map(country_mapping_dict).fillna(df['country'])\n",
    "    \n",
    "    print('Country mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_evars_mapping(df):\n",
    "    \n",
    "    print('Starting custom evars mapping...')\n",
    "    \n",
    "    # load file for custom evars mapping and select columns\n",
    "    evars = pd.read_csv('../data/mapping_files/custom_evars.tsv', sep='\\t')\n",
    "    evars_mapping = evars[['id', 'name']]\n",
    "\n",
    "    # map custom evars\n",
    "    evar_cols = [x for x in df.columns if x.lower()[:9] == 'post_evar']\n",
    "    evar_cols = [x.replace('post_', '') for x in evar_cols]\n",
    "    evars_mapped = evars[evars['id'].isin(evar_cols)][['id', 'name']]\n",
    "    evars_mapped['id'] = evars_mapped['id'].apply(lambda x: 'post_' + x)\n",
    "    evars_mapped = evars_mapped.reset_index(drop=True)\n",
    "\n",
    "    # rename custom evars\n",
    "    for i in range(evars_mapped.shape[0]):\n",
    "        df.rename(columns={evars_mapped.iloc[i,0] : str.lower(evars_mapped.iloc[i,1]).replace(' ','_')}, inplace=True)\n",
    "\n",
    "    print('Custom evars mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_marketing_channel_mapping(df):\n",
    "    \n",
    "    print('Starting custom marketing channel mapping...')\n",
    "    \n",
    "    # load file for custom marketing channel mapping\n",
    "    custom_marketing_channel_mapping = pd.read_csv('../data/mapping_files/custom_marketing_channels.tsv', sep='\\t')\n",
    "\n",
    "    # create dictionary for marketing channel mapping\n",
    "    custom_marketing_channel_mapping_dict = dict(zip(custom_marketing_channel_mapping.channel_id, custom_marketing_channel_mapping.name))\n",
    "\n",
    "    # map custom marketing channels\n",
    "    df['va_closer_id'] = df['va_closer_id'].map(custom_marketing_channel_mapping_dict).fillna(df['va_closer_id'])\n",
    "    df['va_closer_id'] = df['va_closer_id'].apply(lambda x: 'Unknown' if x == 0 else x)\n",
    "\n",
    "    print('Custom marketing channel mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_and_standard_events_mapping(df):\n",
    "    \n",
    "    print('Starting custom and standard events mapping...')\n",
    "\n",
    "    # fill missing values in post_event_list\n",
    "    df['post_event_list'] = df['post_event_list'].fillna('Unknown')\n",
    "\n",
    "    # load file for standard event mapping and select columns\n",
    "    standard_events = pd.read_csv('../data/mapping_files/event.tsv', sep='\\t', header=None)\n",
    "    standard_events.columns = ['event_id', 'event_name']\n",
    "\n",
    "    # load file for custom event mapping and modify event_id for matching\n",
    "    custom_events = pd.read_csv('../data/mapping_files/custom_events.tsv', sep='\\t')\n",
    "    custom_events['event_id'] = custom_events.index + 200\n",
    "\n",
    "    # map standard and custom events\n",
    "    events = pd.merge(standard_events, custom_events, how='inner', on='event_id')\n",
    "    events_mapping = events[['event_id', 'name']]\n",
    "    events_mapping = events_mapping.reset_index(drop=True)\n",
    "\n",
    "    # create event dummies\n",
    "    for id, event in zip(events_mapping.iloc[:,0], events_mapping.iloc[:,1]):\n",
    "            df[str.lower(event).replace(' ','_')] = df['post_event_list'].apply(lambda x: 1 if ','+str(id)+',' in x else 0)\n",
    "\n",
    "    # drop internal users\n",
    "    df = df.drop(df[df['internal_user_(e30)'] == 1].index)\n",
    "\n",
    "    print('Standard and custom events mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def operating_system_mapping(df):\n",
    "    \n",
    "    print('Starting operating system mapping...')\n",
    "    \n",
    "    # load file for operating system mapping and select columns\n",
    "    operating_system_mapping = pd.read_csv('../data/mapping_files/operating_systems.tsv', sep='\\t', header=None)\n",
    "    operating_system_mapping.columns = ['operating_system_id', 'operating_system_name']\n",
    "\n",
    "    # create dictionary for operating system mapping\n",
    "    operating_system_mapping_dict = dict(zip(operating_system_mapping.operating_system_id, operating_system_mapping.operating_system_name))\n",
    "\n",
    "    # map operating systems\n",
    "    df['os'] = df['os'].map(operating_system_mapping_dict).fillna(df['os'])\n",
    "\n",
    "    print('Operating system mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_operating_system(row):\n",
    "    if 'Windows' in row['os']:\n",
    "        return 'Windows'\n",
    "    elif 'Linux' in row['os']:\n",
    "        return 'Linux'\n",
    "    elif 'Android' in row['os']:\n",
    "        return 'Android'\n",
    "    elif 'Mobile iOS' in row['os']:\n",
    "        return 'Apple'\n",
    "    elif 'Macintosh' in row['os']:\n",
    "        return 'Apple'\n",
    "    elif 'OS X' in row['os']:\n",
    "        return 'Apple'\n",
    "    elif 'Not Specified' in row['os']:\n",
    "        return 'Unknown'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def referrer_type_mapping(df):\n",
    "    \n",
    "    print('Starting referrer type mapping...')\n",
    "    \n",
    "    # load file for referrer type mapping and select columns\n",
    "    referrer_type_mapping = pd.read_csv('../data/mapping_files/referrer_type.tsv', sep='\\t', header=None)\n",
    "    referrer_type_mapping.columns = ['referrer_type_id', 'referrer_type_name', 'referrer_type']\n",
    "\n",
    "    # create dictionary for referrer type mapping\n",
    "    referrer_type_mapping_dict = dict(zip(referrer_type_mapping.referrer_type_id, referrer_type_mapping.referrer_type))\n",
    "\n",
    "    # map referrer types\n",
    "    df['ref_type'] = df['ref_type'].map(referrer_type_mapping_dict).fillna(df['ref_type'])\n",
    "\n",
    "    print('Referrer type mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_mapping(df):\n",
    "    \n",
    "    print('Starting search engine mapping...')\n",
    "\n",
    "    # load file for search engine mapping and select columns\n",
    "    search_engine_mapping = pd.read_csv('../data/mapping_files/search_engines.tsv', sep='\\t', header=None)\n",
    "    search_engine_mapping.columns = ['search_engine_id', 'search_engine_name']\n",
    "\n",
    "    # create dictionary for search engine mapping\n",
    "    search_engine_mapping_dict = dict(zip(search_engine_mapping.search_engine_id, search_engine_mapping.search_engine_name))\n",
    "\n",
    "    # map search engines\n",
    "    df['post_search_engine'] = df['post_search_engine'].map(search_engine_mapping_dict).fillna(df['post_search_engine'])\n",
    "\n",
    "    print('Search engine mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generalize_search_engine(row):\n",
    "    if 'Google' in row['post_search_engine']:\n",
    "        return 'Google'\n",
    "    elif 'googleadservices.com' in row['post_search_engine']:\n",
    "        return 'Google'\n",
    "    elif 'Yahoo' in row['post_search_engine']:\n",
    "        return 'Yahoo'\n",
    "    elif 'Bing' in row['post_search_engine']:\n",
    "        return 'Bing'\n",
    "    elif 'Baidu' in row['post_search_engine']:\n",
    "        return 'Baidu'\n",
    "    elif 'DuckDuckGo' in row['post_search_engine']:\n",
    "        return 'DuckDuckGo'\n",
    "    elif 'Yandex' in row['post_search_engine']:\n",
    "        return 'Yandex'\n",
    "    elif 'Search.ch' in row['post_search_engine']:\n",
    "        return 'Search.ch'\n",
    "    elif '0' in row['post_search_engine']:\n",
    "        return 'Unknown'\n",
    "    else:\n",
    "        return 'Other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_agent_mapping(df):\n",
    "    \n",
    "    print('Starting user agent mapping...')\n",
    "    \n",
    "    # fill missing values\n",
    "    df['user_agent'] = df['user_agent'].fillna('Unknown')\n",
    "\n",
    "    # load file for user agent mapping\n",
    "    user_agent_mapping = pd.read_csv('../data/mapping_files/user_agent_mapping.tsv.gz', compression='gzip', sep='\\t', encoding='iso-8859-1', quoting=3, low_memory=False)\n",
    "\n",
    "    # merge user agent mapping and df\n",
    "    df = pd.merge(df, user_agent_mapping, how='left', on='user_agent')\n",
    "\n",
    "    # drop rows where device_is_bot_user_agent == 1\n",
    "    df = df.drop(df[df.device_is_bot_user_agent == 1].index)\n",
    "\n",
    "    print('User agent mapping complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_product_items_prices_and_categories(df):\n",
    "    \n",
    "    print('Starting processing product items, prices and categories...')\n",
    "\n",
    "    # process product items\n",
    "    df['num_product_items_seen'] = df['product_items'].apply(lambda x: 0 if pd.isnull(x) else len(x.split(';')))\n",
    "\n",
    "    # process product prices\n",
    "    df['sum_price_product_items_seen'] = df['product_item_price'].apply(lambda x: 0 if pd.isnull(x)\n",
    "                                                                        else sum([float(i) for i in x.split(';')]))\n",
    "\n",
    "    # process product categories\n",
    "    df['product_categories_level_1'] = df['product_categories'].apply(lambda x: 'Unknown' if pd.isnull(x)\n",
    "                                                                      else [i.split(' / ') for i in x.split(';')][0][0])\n",
    "\n",
    "    df['product_categories_level_2'] = df['product_categories'].apply(lambda x: 'Unknown' if pd.isnull(x) else \n",
    "                                                                      ([i.split(' / ') for i in x.split(';')][0][1] if len([i.split(' / ') for i in x.split(';')][0]) > 1 \n",
    "                                                                      else 'Unknown'))\n",
    "\n",
    "    df['product_categories_level_3'] = df['product_categories'].apply(lambda x: 'Unknown' if pd.isnull(x) else \n",
    "                                                                      ([i.split(' / ') for i in x.split(';')][0][2] if len([i.split(' / ') for i in x.split(';')][0]) > 2 \n",
    "                                                                      else 'Unknown'))\n",
    "\n",
    "    print('Processing product items, prices and categories complete.')\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_user_agent_mapping(input_file, output_file):\n",
    "    \n",
    "    print('Starting generating user agent mapping...')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### LOAD DATA\n",
    "    print('Loading data...')\n",
    "    \n",
    "    # load column headers\n",
    "    column_headers = pd.read_csv('../data/mapping_files/column_headers.tsv', sep='\\t')\n",
    "    \n",
    "    # select columns\n",
    "    columns = ['user_agent']\n",
    "    \n",
    "    # load data\n",
    "    df = pd.read_csv('../data/raw_data/'+input_file, compression='gzip', sep='\\t', encoding='iso-8859-1', quoting=3, low_memory=False, names=column_headers, usecols=columns)\n",
    "    \n",
    "    print('Loading data complete...')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### USER AGENT MAPPING\n",
    "    print('Starting user agent device type, brand name, operating system and bot flag mapping...')\n",
    "    \n",
    "    # fill missing values\n",
    "    df['user_agent'] = df['user_agent'].fillna('Unknown')\n",
    "\n",
    "    # create dataframe for user agent mapping and fill with unique user agents\n",
    "    columns = ['user_agent', \n",
    "               'device_type_user_agent', \n",
    "               'device_brand_name_user_agent', \n",
    "               'device_operating_system_user_agent', \n",
    "               'device_browser_user_agent', \n",
    "               'device_is_bot_user_agent']\n",
    "    index = np.arange(df['user_agent'].nunique())\n",
    "    user_agent_mapping_df = pd.DataFrame(index=index, columns=columns)\n",
    "    user_agent_mapping_df['user_agent'] = df['user_agent'].unique()\n",
    "    \n",
    "    # map device type\n",
    "    user_agent_mapping_df['device_type_user_agent'] = user_agent_mapping_df['user_agent'].apply(lambda x: DeviceDetector(x).parse().device_type())\n",
    "    user_agent_mapping_df['device_type_user_agent'] = user_agent_mapping_df['device_type_user_agent'].apply(lambda x: 'Unknown' if x == '' else x)\n",
    "\n",
    "    # map brand name\n",
    "    user_agent_mapping_df['device_brand_name_user_agent'] = user_agent_mapping_df['user_agent'].apply(lambda x: DeviceDetector(x).parse().device_brand_name())\n",
    "    user_agent_mapping_df['device_brand_name_user_agent'] = user_agent_mapping_df['device_brand_name_user_agent'].apply(lambda x: 'Unknown' if x == 'UNK' else x)\n",
    "\n",
    "    # map operating system\n",
    "    user_agent_mapping_df['device_operating_system_user_agent'] = user_agent_mapping_df['user_agent'].apply(lambda x: DeviceDetector(x).parse().os_name())\n",
    "    user_agent_mapping_df['device_operating_system_user_agent'] = user_agent_mapping_df['device_operating_system_user_agent'].apply(lambda x: 'Unknown' if x == '' else x)\n",
    "\n",
    "    # map browser\n",
    "    user_agent_mapping_df['device_browser_user_agent'] = user_agent_mapping_df['user_agent'].apply(lambda x: DeviceDetector(x).parse().client_name())\n",
    "\n",
    "    # map bot flag\n",
    "    user_agent_mapping_df['device_is_bot_user_agent'] = user_agent_mapping_df['user_agent'].apply(lambda x: DeviceDetector(x).parse().is_bot())\n",
    "    user_agent_mapping_df['device_is_bot_user_agent'] = user_agent_mapping_df['device_is_bot_user_agent'].apply(lambda x: 1 if x == True else 0)\n",
    "\n",
    "    print('User agent device type, brand name, operating system, browser and bot flag mapping complete.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    ### WRITE DATA\n",
    "    print('Starting writing data...')\n",
    "    \n",
    "    user_agent_mapping_df.to_csv('../data/mapping_files/'+output_file, compression='gzip', sep='\\t', encoding='iso-8859-1', index=False)\n",
    "    \n",
    "    print('Writing data complete.')\n",
    "    \n",
    "    \n",
    "    \n",
    "    print('Generating user agent mapping complete.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MISCELLANEOUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_run_time(run_time_dict_file, run_time_dict):\n",
    "    \n",
    "    f = open('../results/descriptives/'+run_time_dict_file, 'w')\n",
    "    f.write(str(run_time_dict))\n",
    "    f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
