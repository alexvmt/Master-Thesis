{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### DATA CLEANING AND PROCESSING #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime,date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start time:  2019-02-12 18:30:07.305698\n"
     ]
    }
   ],
   "source": [
    "start_time = datetime.now()\n",
    "print('Start time: ', start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    }
   ],
   "source": [
    "##### LOAD DATA\n",
    "print('Loading data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex.merdian-tarko\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py:678: UserWarning: Duplicate names specified. This will raise an error in the future.\n",
      "  return _read(filepath_or_buffer, kwds)\n"
     ]
    }
   ],
   "source": [
    "### get column headers and load data into a dataframe\n",
    "column_headers = pd.read_csv('../data/mapping_files/column_headers.tsv', sep='\\t')\n",
    "df = pd.read_csv('../data/raw_data/3_day_sample_raw.tsv.gz', compression='gzip', sep='\\t', encoding='iso-8859-1', quoting=3, low_memory=False, names=column_headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time passed since start: ', datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### CLEAN DATA\n",
    "print('Cleaning data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### drop unnecessary rows\n",
    "# reset index to make sure that index values are unique\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# drop rows where exclude_hit > 1\n",
    "df = df.drop(df[df.exclude_hit > 0].index)\n",
    "\n",
    "# drop rows where hit_source is 5, 7, 8 or 9\n",
    "df = df.drop(df[(df.hit_source == 5) | (df.hit_source == 7) | (df.hit_source == 8) | (df.hit_source == 9)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### browser mapping\n",
    "# load file for browser mapping\n",
    "browser_mapping = pd.read_csv('../data/mapping_files/browser_type.tsv', sep='\\t', header=None)\n",
    "browser_mapping.columns = ['browser_id', 'browser_name']\n",
    "\n",
    "# create dictionary for browser mapping\n",
    "browser_mapping_dict = dict(zip(browser_mapping.browser_id, browser_mapping.browser_name))\n",
    "\n",
    "# map browsers\n",
    "df['browser'] = df['browser'].map(browser_mapping_dict).fillna(df['browser'])\n",
    "df['browser'] = df['browser'].apply(lambda x: 'Not Specified' if x == 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### connection type mapping\n",
    "# load file for connection type mapping\n",
    "connection_type_mapping = pd.read_csv('../data/mapping_files/connection_type.tsv', sep='\\t', header=None)\n",
    "connection_type_mapping.columns = ['connection_type_id', 'connection_type_name']\n",
    "\n",
    "# create dictionary for connection type mapping\n",
    "connection_type_mapping_dict = dict(zip(connection_type_mapping.connection_type_id, connection_type_mapping.connection_type_name))\n",
    "\n",
    "# map connection types\n",
    "df['connection_type'] = df['connection_type'].map(connection_type_mapping_dict).fillna(df['connection_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### country mapping\n",
    "# load file for country mapping\n",
    "country_mapping = pd.read_csv('../data/mapping_files/country.tsv', sep='\\t', header=None)\n",
    "country_mapping.columns = ['country_id', 'country_name']\n",
    "\n",
    "# drop dupliate countries\n",
    "country_mapping = country_mapping.drop_duplicates('country_name').reset_index(drop=True)\n",
    "\n",
    "# create dictionary for country mapping\n",
    "country_mapping_dict = dict(zip(country_mapping.country_id, country_mapping.country_name))\n",
    "\n",
    "# map countries\n",
    "df['country'] = df['country'].map(country_mapping_dict).fillna(df['country'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom evars mapping\n",
    "# load file for custom evars mapping\n",
    "evars = pd.read_csv('../data/mapping_files/custom_evars.tsv', sep='\\t')\n",
    "evars_mapping = evars[['id', 'name']]\n",
    "\n",
    "# map custom evars\n",
    "evar_cols = [x for x in df.columns if x.lower()[:9] == 'post_evar']\n",
    "evar_cols = [x.replace('post_', '') for x in evar_cols]\n",
    "evars_mapped = evars[evars['id'].isin(evar_cols)][['id', 'name']]\n",
    "evars_mapped['id'] = evars_mapped['id'].apply(lambda x: 'post_' + x)\n",
    "evars_mapped = evars_mapped.reset_index(drop=True)\n",
    "\n",
    "# rename custom evars\n",
    "for i in range(evars_mapped.shape[0]):\n",
    "    df.rename(columns={evars_mapped.iloc[i,0] : str.lower(evars_mapped.iloc[i,1]).replace(' ','_')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom marketing channel mapping\n",
    "# load file for marketing channel mapping\n",
    "marketing_channel_mapping = pd.read_csv('../data/mapping_files/custom_marketing_channels.tsv', sep='\\t')\n",
    "\n",
    "# create dictionary for marketing channel mapping\n",
    "marketing_channel_mapping_dict = dict(zip(marketing_channel_mapping.channel_id, marketing_channel_mapping.name))\n",
    "\n",
    "# map marketing channels\n",
    "df['marketing_channel'] = df['va_closer_id'].map(marketing_channel_mapping_dict).fillna(df['va_closer_id'])\n",
    "df['marketing_channel'] = df['marketing_channel'].apply(lambda x: 'Not Specified' if x == 0 else x)\n",
    "df.drop('va_closer_id', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom events and standard events mapping\n",
    "# fill missing values in post_event_list\n",
    "df['post_event_list'] = df['post_event_list'].fillna('Not Specified')\n",
    "\n",
    "# load file for event mapping\n",
    "standard_events = pd.read_csv('../data/mapping_files/event.tsv', sep='\\t', header=None)\n",
    "standard_events.columns = ['event_id', 'event_name']\n",
    "\n",
    "# load file for custom event mapping\n",
    "custom_events = pd.read_csv('../data/mapping_files/custom_events.tsv', sep='\\t')\n",
    "custom_events['event_id'] = custom_events.index + 200\n",
    "\n",
    "# map events and custom events\n",
    "events = pd.merge(standard_events, custom_events, how='inner', on='event_id')\n",
    "events_mapping = events[['event_id', 'name']]\n",
    "events_mapping = events_mapping.reset_index(drop=True)\n",
    "\n",
    "# create event dummies\n",
    "for id, event in zip(events_mapping.iloc[:,0], events_mapping.iloc[:,1]):\n",
    "        df[str.lower(event).replace(' ','_')] = df['post_event_list'].apply(lambda x: 1 if ','+str(id)+',' in x else 0)\n",
    "        \n",
    "# drop internal users\n",
    "df = df.drop(df[df['internal_user_(e30)'] == 1].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### custom props mapping\n",
    "# load file for custom props mapping\n",
    "props = pd.read_csv('../data/mapping_files/custom_props.tsv', sep='\\t')\n",
    "props_mapping = props[['id', 'name']]\n",
    "\n",
    "# map custom evars\n",
    "prop_cols = [x for x in df.columns if x.lower()[:9] == 'post_prop']\n",
    "prop_cols = [x.replace('post_', '') for x in prop_cols]\n",
    "props_mapped = props[props['id'].isin(prop_cols)][['id', 'name']]\n",
    "props_mapped['id'] = props_mapped['id'].apply(lambda x: 'post_' + x)\n",
    "props_mapped = props_mapped.reset_index(drop=True)\n",
    "\n",
    "# rename custom props\n",
    "for i in range(props_mapped.shape[0]):\n",
    "    df.rename(columns={props_mapped.iloc[i,0] : str.lower(props_mapped.iloc[i,1]).replace(' ','_')}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### operating system mapping\n",
    "# load file for operating system mapping\n",
    "operating_system_mapping = pd.read_csv('../data/mapping_files/operating_systems.tsv', sep='\\t', header=None)\n",
    "operating_system_mapping.columns = ['operating_system_id', 'operating_system_name']\n",
    "\n",
    "# create dictionary for operating system mapping\n",
    "operating_system_mapping_dict = dict(zip(operating_system_mapping.operating_system_id, operating_system_mapping.operating_system_name))\n",
    "\n",
    "# map operating systems\n",
    "df['operating_system'] = df['os'].map(operating_system_mapping_dict).fillna(df['os'])\n",
    "df.drop('os', axis=1, inplace=True)\n",
    "\n",
    "# generalize operating system\n",
    "def generalize_operating_system(row):\n",
    "    if 'Windows' in row['operating_system']:\n",
    "        return 'Windows'\n",
    "    elif 'Linux' in row['operating_system']:\n",
    "        return 'Linux'\n",
    "    elif 'Android' in row['operating_system']:\n",
    "        return 'Android'\n",
    "    elif 'Mobile iOS' in row['operating_system']:\n",
    "        return 'Apple'\n",
    "    elif 'Macintosh' in row['operating_system']:\n",
    "        return 'Apple'\n",
    "    elif 'OS X' in row['operating_system']:\n",
    "        return 'Apple'\n",
    "    elif 'Not Specified' in row['operating_system']:\n",
    "        return 'Not Specified'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "df['operating_system_generalized'] = df.apply(generalize_operating_system, axis=1)\n",
    "df.drop('operating_system', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### referrer type mapping\n",
    "# load file for referrer type mapping\n",
    "referrer_type_mapping = pd.read_csv('../data/mapping_files/referrer_type.tsv', sep='\\t', header=None)\n",
    "referrer_type_mapping.columns = ['referrer_type_id', 'referrer_type_name', 'referrer_type']\n",
    "\n",
    "# create dictionary for referrer type mapping\n",
    "referrer_type_mapping_dict = dict(zip(referrer_type_mapping.referrer_type_id, referrer_type_mapping.referrer_type))\n",
    "\n",
    "# map referrer types\n",
    "df['referrer_type'] = df['ref_type'].map(referrer_type_mapping_dict).fillna(df['ref_type'])\n",
    "df.drop('ref_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### search engine mapping\n",
    "# load file for search engine mapping\n",
    "search_engine_mapping = pd.read_csv('../data/mapping_files/search_engines.tsv', sep='\\t', header=None)\n",
    "search_engine_mapping.columns = ['search_engine_id', 'search_engine_name']\n",
    "\n",
    "# create dictionary for search engine mapping\n",
    "search_engine_mapping_dict = dict(zip(search_engine_mapping.search_engine_id, search_engine_mapping.search_engine_name))\n",
    "\n",
    "# map search engines\n",
    "df['search_engine'] = df['post_search_engine'].map(search_engine_mapping_dict).fillna(df['post_search_engine'])\n",
    "df.drop('post_search_engine', axis=1, inplace=True)\n",
    "\n",
    "# convert search engine column to string type\n",
    "df['search_engine'] = df['search_engine'].astype(str)\n",
    "\n",
    "# generalize search engine\n",
    "def generalize_search_engine(row):\n",
    "    if 'Google' in row['search_engine']:\n",
    "        return 'Google'\n",
    "    elif 'Yahoo' in row['search_engine']:\n",
    "        return 'Yahoo'\n",
    "    elif 'Bing' in row['search_engine']:\n",
    "        return 'Bing'\n",
    "    elif 'Baidu' in row['search_engine']:\n",
    "        return 'Baidu'\n",
    "    elif 'DuckDuckGo' in row['search_engine']:\n",
    "        return 'DuckDuckGo'\n",
    "    elif 'Yandex' in row['search_engine']:\n",
    "        return 'Yandex'\n",
    "    elif 'Search.ch' in row['search_engine']:\n",
    "        return 'Search.ch'\n",
    "    elif '0' in row['search_engine']:\n",
    "        return ' Not Specified'\n",
    "    else:\n",
    "        return 'Other'\n",
    "    \n",
    "df['search_engine_generalized'] = df.apply(generalize_search_engine, axis=1)\n",
    "df.drop('search_engine', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### split and process product_items, product_item_price and product_categories columns\n",
    "df['num_product_items_seen'] = df['product_items'].apply(lambda x: 0 if pd.isnull(x) else len(x.split(';')))\n",
    "\n",
    "df['sum_price_product_items_seen'] = df['product_item_price'].apply(lambda x: 0 if pd.isnull(x)\n",
    "                                                              else sum([float(i) for i in x.split(';')]))\n",
    "\n",
    "df['product_categories_level_1'] = df['product_categories'].apply(lambda x: 'Not Specified' if pd.isnull(x)\n",
    "                                                                  else [i.split(' / ') for i in x.split(';')][0][0])\n",
    "\n",
    "df['product_categories_level_2'] = df['product_categories'].apply(lambda x: 'Not Specified' if pd.isnull(x) else \n",
    "                                                                  ([i.split(' / ') for i in x.split(';')][0][1] if len([i.split(' / ') for i in x.split(';')][0]) > 1 \n",
    "                                                                  else 'Not Specified'))\n",
    "\n",
    "df['product_categories_level_3'] = df['product_categories'].apply(lambda x: 'Not Specified' if pd.isnull(x) else \n",
    "                                                                  ([i.split(' / ') for i in x.split(';')][0][2] if len([i.split(' / ') for i in x.split(';')][0]) > 2 \n",
    "                                                                  else 'Not Specified'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time passed since start: ', datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### AGGREGATE NUMERICAL COLUMNS TO SESSION LEVEL\n",
    "print('Aggregating numerical columns...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### aggregate numerical columns\n",
    "# convert hit_time_gmt column from unix format to datetime format\n",
    "df['hit_time_gmt'] = pd.to_datetime(df['hit_time_gmt'], unit='s')\n",
    "\n",
    "# convert visit_page_num column to integer type\n",
    "df['visit_page_num'] = df['visit_page_num'].astype(np.int64)\n",
    "\n",
    "# fill missing values with 0 in cart_value_(v50) column\n",
    "df['cart_value_(v50)'].fillna(0, inplace=True)\n",
    "\n",
    "# select numerical columns\n",
    "numerical_cols_names = ['visitor_id', 'visit_num', 'visit_page_num', 'hit_time_gmt' ,'purchase_boolean', \n",
    "                        'product_view_boolean', 'checkout_boolean', 'cart_addition_boolean', 'cart_removal_boolean', \n",
    "                        'cart_view_boolean', 'campaign_view_boolean', 'cart_value_(v50)', 'page_view_boolean', \n",
    "                        'last_purchase_num', 'num_product_items_seen', 'sum_price_product_items_seen']\n",
    "numerical_cols = df.loc[:, df.columns.isin(numerical_cols_names)].copy()\n",
    "\n",
    "# group by visitor_id and visit_num and aggregate numerical columns\n",
    "numerical_cols_aggregated = numerical_cols.groupby(by = ['visitor_id', 'visit_num'], as_index=False).agg({'visit_page_num' : 'max',\n",
    "                                                                                                          'hit_time_gmt': ['min', 'max'],\n",
    "                                                                                                          'purchase_boolean' : 'sum',\n",
    "                                                                                                          'product_view_boolean' : 'sum',\n",
    "                                                                                                          'checkout_boolean' : 'sum',\n",
    "                                                                                                          'cart_addition_boolean': 'sum',\n",
    "                                                                                                          'cart_removal_boolean': 'sum',\n",
    "                                                                                                          'cart_view_boolean': 'sum',\n",
    "                                                                                                          'campaign_view_boolean': 'sum',\n",
    "                                                                                                          'cart_value_(v50)': 'sum',\n",
    "                                                                                                          'page_view_boolean': 'sum',\n",
    "                                                                                                          'last_purchase_num': 'max',\n",
    "                                                                                                          'num_product_items_seen' : 'sum',\n",
    "                                                                                                          'sum_price_product_items_seen' : 'sum'})\n",
    "numerical_cols_aggregated.columns = list(numerical_cols_aggregated.columns)\n",
    "numerical_cols_aggregated.columns = ['visitor_id', 'visit_num', 'visit_page_num', 'hit_time_gmt', 'last_hit_time_gmt_visit', \n",
    "                                     'purchase', 'product_view', 'checkout', 'cart_addition', 'cart_removal', 'cart_view', \n",
    "                                     'campaign_view', 'cart_value', 'page_view', 'last_purchase_num', \n",
    "                                     'num_product_items_seen', 'sum_price_product_items_seen']\n",
    "\n",
    "# sort dataframe by hit_time_gmt, last_hit_time_gmt_visit, visitor_id and visit_num\n",
    "numerical_cols_aggregated = numerical_cols_aggregated.sort_values(['hit_time_gmt', 'last_hit_time_gmt_visit', 'visitor_id', 'visit_num'], ascending=[True, True, True, True])\n",
    "numerical_cols_aggregated = numerical_cols_aggregated.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time passed since start: ', datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### PROCESS CATEGORICAL COLUMNS\n",
    "print('Processing categorical columns...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select categorical columns\n",
    "categorical_cols_names = ['visitor_id', 'hit_time_gmt', 'connection_type', 'country', 'geo_city', 'geo_dma', \n",
    "                          'geo_region', 'geo_zip', 'post_channel', 'post_cookies', 'net_promoter_score_raw_(v10)_-_user', \n",
    "                          'registered_user_(user)_(v34)', 'login_status_(hit)_(v37)', 'user_gender_(v61)', 'user_age_(v62)',\n",
    "                          'post_persistent_cookie', 'browser_generalized', 'operating_system_generalized',\n",
    "                          'search_engine_generalized', 'marketing_channel', 'referrer_type', 'hit_of_logged_in_user_(e23)',\n",
    "                          'visit_during_tv_spot_(e71)', 'repeat_orders_(e9)', 'newsletter_subscriber_(e27)', \n",
    "                          'registration_(any_form)_(e20)', 'registration_fail_(e75)', 'newsletter_signup_(any_form)_(e26)', \n",
    "                          'new_visit', 'hourly_visitor', 'daily_visitor', 'weekly_visitor', 'monthly_visitor', \n",
    "                          'quarterly_visitor', 'yearly_visitor', 'product_categories_level_1', 'product_categories_level_2',\n",
    "                          'product_categories_level_3']\n",
    "categorical_cols = df.loc[:, df.columns.isin(categorical_cols_names)].copy()\n",
    "\n",
    "# transform categorical columns where necessary\n",
    "categorical_cols['post_cookies'] = categorical_cols['post_cookies'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "categorical_cols['registered_user'] = categorical_cols['registered_user_(user)_(v34)'].apply(lambda x: 1 if x == 'y' else 0)\n",
    "categorical_cols.drop('registered_user_(user)_(v34)', axis=1, inplace=True)\n",
    "categorical_cols['login_status'] = categorical_cols['login_status_(hit)_(v37)'].apply(lambda x: 1 if x == 'y' else 0)\n",
    "categorical_cols.drop('login_status_(hit)_(v37)', axis=1, inplace=True)\n",
    "categorical_cols['post_persistent_cookie'] = categorical_cols['post_persistent_cookie'].apply(lambda x: 1 if x == 'Y' else 0)\n",
    "categorical_cols['geo_city'] = categorical_cols['geo_city'].apply(lambda x: 'Not Specified' if x == '?' else x)\n",
    "categorical_cols['geo_region'] = categorical_cols['geo_region'].apply(lambda x: 'Not Specified' if x == '?' else x)\n",
    "\n",
    "# static columns: registration_(any_form)_(e20), newsletter_signup_(any_form)_(e26), newsletter_subscriber_(e27), registration_fail_(e75)\n",
    "# columns with lots of missing values: NPS, gender, age\n",
    "\n",
    "# sort dataframe by hit_time_gmt_min, visitor_id and visit_num\n",
    "categorical_cols = categorical_cols.sort_values(['hit_time_gmt', 'visitor_id'], ascending=[True, True])\n",
    "categorical_cols = categorical_cols.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time passed since start: ', datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### MERGE NUMERICAL AND CATEGORICAL COLUMNS ON SESSION LEVEL\n",
    "print('Merging numerical and categorical columns...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_level_data_merged = pd.merge_asof(numerical_cols_aggregated, categorical_cols, on='hit_time_gmt', by='visitor_id')\n",
    "session_level_data_merged = session_level_data_merged.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Time passed since start: ', datetime.now() - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### WRITE DATA TO FILE\n",
    "print('Writing data to file...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write numerical and categorical columns merged to session-level dataframe to tsv file\n",
    "session_level_data_merged.to_csv('../data/processed_data/session_level_data_merged.csv', sep=',', encoding='iso-8859-1', index=False, header=True, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Total execution time: ', datetime.now() - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
